{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will introduce the evaluation metrics used in our models. We used several metrics to see how they can give us different insights about the performance of our models and we came to this conclusion:\n",
    "\n",
    "- **R2 Score**: This metric is mostly used when we don't the scale of the target variable. It is a good metric to see how well our model is performing compared to a simple model that predicts the mean of the target variable. The problem was we got a very high R2 score for all of our models and the reason is that the mean squared error when we predict the mean of the target variable is very high so even if our model is not performing well, the R2 score will be high. \n",
    "\n",
    "$$ R^2 = 1 - \\frac{MSE_{model}}{MSE_{mean}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n",
    "\n",
    "We check the RMSE of the mean model to illustrate this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error when we guess the mean: 1418975.7503518122\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv('../data/not_scaled_data.csv')\n",
    "\n",
    "y_true = df['price']\n",
    "y_pred = [df['price'].mean()]*len(df['price'])\n",
    "\n",
    "print('Root Mean Squared Error when we guess the mean:', np.sqrt(mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error when we guess the mean: 0.273661211274132\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv('../data/not_scaled_data.csv')\n",
    "\n",
    "y_true = df['log_price']\n",
    "y_pred = [df['log_price'].mean()]*len(df['log_price'])\n",
    "\n",
    "print('Root Mean Squared Error when we guess the mean:', np.sqrt(mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the RMSE for `price` is 1.418 Millions and for `log_price` is 0.27 which is very high and this is why the R2 score is very high for all of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Root Mean Squared Error (RMSE), Mean Absolute Error and Median Absolute Error**: We report these metrics to see how much error our model is making in predicting the target variable. The difference between RMSE and Mean Absolute Error tells us if we predicted some of the prices very badly compared to the rest of the prices. The Median Absolute Error is also good to see when our model predict so badly on some few prices but in general it's better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Mean Absolute Percentage Error and Median Absolute Percentage Error**: These metrics are good to see how much percentage error our model is making in predicting the target variable. Because if we have an error of 10000 on a house that costs 100000 it's not the same as having an error of 10000 on a house that costs 1000000 but the previous metrics will treat them the same. Again we use both mean and median to see if our model is making a big error on some few prices.\n",
    "\n",
    "$$ MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i} $$\n",
    "\n",
    "* **Within x%**: This metric is used to see how many of our predictions are within a certain percentage of the actual price. This is a good metric to see how well our model is performing in general. Zillow itself uses this metric alongside Median Absolute Percentage Error to evaluate the performance of the models.([see this for more information](https://www.zillow.com/tech/home-value-estimates/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined the metrics we will use, We implement some functions to show these metrics for our models.\n",
    "\n",
    "`show_metrics` function gets the predictions and the actual values and if the target variable is `price` or `log_price` and if the target variable was scaled or not and it will show the metrics we defined above in a neat dataframe. So now we can use it for all of our models to see how they are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def calc_median_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_metrics(y_pred, y_test):\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    smse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    mean_ae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    mean_ape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
    "    median_ae = metrics.median_absolute_error(y_test, y_pred)\n",
    "    median_ape = calc_median_absolute_percentage_error(y_test, y_pred)\n",
    "    return [r2, smse, mean_ae, mean_ape, median_ae, median_ape]\n",
    "    \n",
    "def within_x_percent(y_pred, y_test, x):\n",
    "    return np.sum(np.abs((y_pred - y_test)/y_test) < x) / len(y_test) * 100\n",
    "\n",
    "def show_metrics(y_pred, y_test, target_scaler = None, logarithm = False):        \n",
    "    metrics_df = pd.DataFrame(columns=['Target', 'R2', 'Root Mean Squared Error', 'Mean Absolute Error',\n",
    "                                            'Mean Absolute Percentage Error', 'Median Absolute Error', 'Median Absolute Percentage Error'])\n",
    "    \n",
    "    metrics_df.loc[0] = ['Target is scaled'] + calculate_metrics(y_pred, y_test)\n",
    "    \n",
    "    if target_scaler:\n",
    "        y_pred = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "        y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "        metrics_df.loc[1] = ['Scaled Target is inversed to real value'] + calculate_metrics(y_pred, y_test)\n",
    "\n",
    "    \n",
    "    if logarithm:\n",
    "        y_pred = np.power(10, y_pred)\n",
    "        y_test = np.power(10, y_test)\n",
    "        metrics_df.loc[2] = ['Target -> 10 ^ Target'] + calculate_metrics(y_pred, y_test)\n",
    "    \n",
    "    dist_df = pd.DataFrame({\"within 5%\": [within_x_percent(y_pred, y_test, 0.05)],\n",
    "                    \"within 10%\": [within_x_percent(y_pred, y_test, 0.10)],\n",
    "                    \"within 20%\": [within_x_percent(y_pred, y_test, 0.20)],\n",
    "                    \"within 50%\": [within_x_percent(y_pred, y_test, 0.50)],\n",
    "                    \"median absolute percentage error\": [calc_median_absolute_percentage_error(y_test, y_pred)]}, index=['Percentage'])\n",
    "        \n",
    "    display(metrics_df)\n",
    "    display(dist_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
